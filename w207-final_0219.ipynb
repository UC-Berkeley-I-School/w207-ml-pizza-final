{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"**W207 Final Final Project Team 2 - Random Acts of Pizza**\n\nTeam Members: Ahmad Azizi, Jordan Thomas, Prashant K Dhingra, Qi Yao","metadata":{}},{"cell_type":"markdown","source":"**Problem Description:**\n\nThis dataset includes 5671 requests collected from the Reddit community Random Acts of Pizza between December 8, 2010 and September 29, 2013 (retrieved on September 30, 2013). All requests ask for the same thing: a free pizza. The outcome of each request -- whether its author received a pizza or not -- is known. Meta-data includes information such as: time of the request, activity of the requester, community-age of the requester, etc.\n\nThe objective of the competition was to create an algorithm capable of predicting which requests will garner a cheesy (but sincere!) act of kindness.\n\nThe data are stored in JSON format. Each JSON entry corresponds to one request (the first and only request by the requester on Random Acts of Pizza).","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport json\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nimport re\nfrom sklearn.feature_extraction.text import *\nimport nltk\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer\nfrom nltk.stem import WordNetLemmatizer\nnltk.download('stopwords')\nnltk.download('punkt')\nnltk.download('wordnet')\n\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import r2_score\nfrom xgboost import XGBClassifier\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn import metrics\n\nfrom sklearn.metrics import accuracy_score\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-02-19T16:59:55.800495Z","iopub.execute_input":"2022-02-19T16:59:55.801482Z","iopub.status.idle":"2022-02-19T16:59:58.358727Z","shell.execute_reply.started":"2022-02-19T16:59:55.801348Z","shell.execute_reply":"2022-02-19T16:59:58.357529Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"#load json train and test data, print the shape\n#Train has more columns than test\nwith open('../input/random-acts-of-pizza/train.json') as train_data:\n    traindata = json.load(train_data)\ntrain = pd.json_normalize(traindata)\n\nwith open('../input/random-acts-of-pizza/test.json') as test_data:\n    testdata = json.load(test_data)\ntest = pd.json_normalize(testdata)\n\nprint('Train shape: ', train.shape)\nprint('Test shape: ', test.shape)","metadata":{"execution":{"iopub.status.busy":"2022-02-19T16:59:58.363363Z","iopub.execute_input":"2022-02-19T16:59:58.363660Z","iopub.status.idle":"2022-02-19T16:59:59.041752Z","shell.execute_reply.started":"2022-02-19T16:59:58.363627Z","shell.execute_reply":"2022-02-19T16:59:59.040817Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"**Exploratory Analysis**","metadata":{}},{"cell_type":"code","source":"train.head()\n#request_test_edit_aware, requester_subreddits_at_request and request-title are the text features\n#rest are all numeric features","metadata":{"execution":{"iopub.status.busy":"2022-02-19T16:59:59.043296Z","iopub.execute_input":"2022-02-19T16:59:59.043878Z","iopub.status.idle":"2022-02-19T16:59:59.097073Z","shell.execute_reply.started":"2022-02-19T16:59:59.043808Z","shell.execute_reply":"2022-02-19T16:59:59.096069Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"train.columns","metadata":{"execution":{"iopub.status.busy":"2022-02-19T16:59:59.100509Z","iopub.execute_input":"2022-02-19T16:59:59.100867Z","iopub.status.idle":"2022-02-19T16:59:59.111510Z","shell.execute_reply.started":"2022-02-19T16:59:59.100831Z","shell.execute_reply":"2022-02-19T16:59:59.110343Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"test.columns\n#test data is only a subset of the train data","metadata":{"execution":{"iopub.status.busy":"2022-02-19T16:59:59.113500Z","iopub.execute_input":"2022-02-19T16:59:59.114355Z","iopub.status.idle":"2022-02-19T16:59:59.129296Z","shell.execute_reply.started":"2022-02-19T16:59:59.114294Z","shell.execute_reply":"2022-02-19T16:59:59.127976Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"#no null value for what we are using\ntrain.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2022-02-19T16:59:59.132524Z","iopub.execute_input":"2022-02-19T16:59:59.135228Z","iopub.status.idle":"2022-02-19T16:59:59.161686Z","shell.execute_reply.started":"2022-02-19T16:59:59.135154Z","shell.execute_reply":"2022-02-19T16:59:59.160450Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"#no null on test data as well\ntest.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2022-02-19T16:59:59.167665Z","iopub.execute_input":"2022-02-19T16:59:59.168596Z","iopub.status.idle":"2022-02-19T16:59:59.182091Z","shell.execute_reply.started":"2022-02-19T16:59:59.168554Z","shell.execute_reply":"2022-02-19T16:59:59.180843Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"train_sns = train_A = train[['requester_account_age_in_days_at_request', 'requester_days_since_first_post_on_raop_at_request', \n                     'requester_number_of_comments_at_request', 'requester_number_of_comments_in_raop_at_request', \n                     'requester_number_of_posts_at_request', 'requester_number_of_posts_on_raop_at_request', \n                     'requester_number_of_subreddits_at_request',  \n                     'requester_upvotes_minus_downvotes_at_request', 'requester_upvotes_plus_downvotes_at_request', \n                             'requester_received_pizza']]","metadata":{"execution":{"iopub.status.busy":"2022-02-19T16:59:59.184118Z","iopub.execute_input":"2022-02-19T16:59:59.184875Z","iopub.status.idle":"2022-02-19T16:59:59.201533Z","shell.execute_reply.started":"2022-02-19T16:59:59.184822Z","shell.execute_reply":"2022-02-19T16:59:59.199449Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"#does not seem the features are very separatable\nsns.pairplot(train_sns,hue='requester_received_pizza')","metadata":{"execution":{"iopub.status.busy":"2022-02-19T16:59:59.205805Z","iopub.execute_input":"2022-02-19T16:59:59.207337Z","iopub.status.idle":"2022-02-19T17:00:43.448391Z","shell.execute_reply.started":"2022-02-19T16:59:59.207271Z","shell.execute_reply":"2022-02-19T17:00:43.444579Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"**Feature Engineering:**\n\nBased on the initial observation, we have mostly numeric columns, and three text columns.\n\nNumeric features and text features (categorical features) will need to be processed separately.\n\n","metadata":{}},{"cell_type":"code","source":"#create label set and convert true/false value to 1/0\ntrain_label = train['requester_received_pizza'].astype('int')","metadata":{"execution":{"iopub.status.busy":"2022-02-19T17:00:43.451075Z","iopub.execute_input":"2022-02-19T17:00:43.451829Z","iopub.status.idle":"2022-02-19T17:00:43.458113Z","shell.execute_reply.started":"2022-02-19T17:00:43.451786Z","shell.execute_reply":"2022-02-19T17:00:43.457064Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"#train_A is a subset of features that are relelvant minus the text features, B, C  each is a text feature\n#we need to feature engineer or train them separately\ntrain_A = train[['requester_account_age_in_days_at_request', 'requester_days_since_first_post_on_raop_at_request', \n                     'requester_number_of_comments_at_request', 'requester_number_of_comments_in_raop_at_request', \n                     'requester_number_of_posts_at_request', 'requester_number_of_posts_on_raop_at_request', \n                     'requester_number_of_subreddits_at_request',  \n                     'requester_upvotes_minus_downvotes_at_request', 'requester_upvotes_plus_downvotes_at_request']]\ntrain_B = train['request_text_edit_aware']\ntrain_C = train['request_title']\n\n#repeat for test\ntest_A = test[['requester_account_age_in_days_at_request', 'requester_days_since_first_post_on_raop_at_request', \n                     'requester_number_of_comments_at_request', 'requester_number_of_comments_in_raop_at_request', \n                     'requester_number_of_posts_at_request', 'requester_number_of_posts_on_raop_at_request', \n                     'requester_number_of_subreddits_at_request',  \n                     'requester_upvotes_minus_downvotes_at_request', 'requester_upvotes_plus_downvotes_at_request']]\ntest_B = test['request_text_edit_aware']\ntest_C = test['request_title']\n","metadata":{"execution":{"iopub.status.busy":"2022-02-19T17:00:43.460053Z","iopub.execute_input":"2022-02-19T17:00:43.460396Z","iopub.status.idle":"2022-02-19T17:00:43.475480Z","shell.execute_reply.started":"2022-02-19T17:00:43.460357Z","shell.execute_reply":"2022-02-19T17:00:43.474295Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"#for column requester_subreddits_at_request, convert it to three values, either no data, has random acts of pizza, or others\n#make it more meaningful for the model\ntrain_sub = []\nsub = train['requester_subreddits_at_request']\nfor i in sub:    \n    if len(i) == 0:\n        i='none'\n    elif 'Random_Acts_Of_Pizza' in i:\n        i='flagged'\n    else:\n        i='nonflagged'\n    train_sub.append(i)","metadata":{"execution":{"iopub.status.busy":"2022-02-19T17:00:43.478133Z","iopub.execute_input":"2022-02-19T17:00:43.478994Z","iopub.status.idle":"2022-02-19T17:00:43.498624Z","shell.execute_reply.started":"2022-02-19T17:00:43.478932Z","shell.execute_reply":"2022-02-19T17:00:43.497781Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"#function to pre process the text data\n#use lemm, but can also try different combination of stop words,stem vs lemm, remove words\ndef pre_processing(data):\n    stop_words = set(stopwords.words('english')) #use stop words\n    #data = re.sub('(\\d+)', '', data) #remove digits\n    data = re.sub('\\W+',' ', data) #remove special chaacters\n    word_tokens = word_tokenize(data.lower()) #tokenize the string\n    filtered_sentence = [w for w in word_tokens if not w.lower() in stop_words] #remove stop words\n    #porter_stemmer = PorterStemmer() #stem the words \n    #stemmed_words = [porter_stemmer.stem(word) for word in filtered_sentence]\n    wordnet_lemmatizer = WordNetLemmatizer() #lemm the word, does better than stem the word\n    lemm_words = [wordnet_lemmatizer.lemmatize(word) for word in filtered_sentence]\n    return ' '.join(lemm_words)","metadata":{"execution":{"iopub.status.busy":"2022-02-19T17:00:43.500165Z","iopub.execute_input":"2022-02-19T17:00:43.500664Z","iopub.status.idle":"2022-02-19T17:00:43.510173Z","shell.execute_reply.started":"2022-02-19T17:00:43.500630Z","shell.execute_reply":"2022-02-19T17:00:43.509334Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"#pre process the text data (vectorizer)\nvectorizer_proc = CountVectorizer(preprocessor = pre_processing)\ntrain_B_proc = vectorizer_proc.fit_transform(train_B)\ntrain_B_proc = pd.DataFrame(train_B_proc.todense())\ntrain_B_columns = vectorizer_proc.get_feature_names()\ntrain_C_proc = vectorizer_proc.fit_transform(train_C)\ntrain_C_proc = pd.DataFrame(train_C_proc.todense())\n#train_C_columns = vectorizer_proc.get_feature_names()","metadata":{"execution":{"iopub.status.busy":"2022-02-19T17:00:43.512147Z","iopub.execute_input":"2022-02-19T17:00:43.512656Z","iopub.status.idle":"2022-02-19T17:00:53.268235Z","shell.execute_reply.started":"2022-02-19T17:00:43.512620Z","shell.execute_reply":"2022-02-19T17:00:53.267316Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"#standardize the numeric value columns\nsc = StandardScaler()\ntrain_A_norm = sc.fit_transform(train_A)\ntrain_A_norm = pd.DataFrame(train_A_norm)","metadata":{"execution":{"iopub.status.busy":"2022-02-19T17:00:53.269690Z","iopub.execute_input":"2022-02-19T17:00:53.269945Z","iopub.status.idle":"2022-02-19T17:00:53.282649Z","shell.execute_reply.started":"2022-02-19T17:00:53.269901Z","shell.execute_reply":"2022-02-19T17:00:53.281657Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"#use number to name each column so these are all unique\n#can use column index to find text column (feature) name if needed for further analysis\ntrain_A_norm.columns = [str(i) for i in range(3,12)]\ntrain_B_proc.columns = [str(i) for i in range(12,11052)]\ntrain_C_proc.columns = [str(i) for i in range(11052,15128)]\n#train_A_norm.columns = train_A_norm_columns\n#train_B_proc.columns = train_B_columns\n#train_C_proc.columns = train_C_columns","metadata":{"execution":{"iopub.status.busy":"2022-02-19T17:00:53.284071Z","iopub.execute_input":"2022-02-19T17:00:53.284404Z","iopub.status.idle":"2022-02-19T17:00:53.295552Z","shell.execute_reply.started":"2022-02-19T17:00:53.284358Z","shell.execute_reply":"2022-02-19T17:00:53.294872Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"#one hot encoding the train_sub data which is categorical \nenc = OneHotEncoder(handle_unknown='ignore')\ntrain_sub_one = enc.fit_transform(np.array(train_sub).reshape(-1,1))\ntrain_sub_one = pd.DataFrame(train_sub_one.todense())\n#train_sub_one.columns = list(enc.get_feature_names())","metadata":{"execution":{"iopub.status.busy":"2022-02-19T17:00:53.296844Z","iopub.execute_input":"2022-02-19T17:00:53.297199Z","iopub.status.idle":"2022-02-19T17:00:53.314486Z","shell.execute_reply.started":"2022-02-19T17:00:53.297169Z","shell.execute_reply":"2022-02-19T17:00:53.313612Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"#combine all of them and do train test split using 0.9 ratio for the model\ntrain_all = pd.concat([train_B_proc,train_C_proc, train_sub_one, train_A_norm], axis=1, sort=False)\nX_train, X_test, Y_train, Y_test = train_test_split(train_all, train_label, train_size = 0.9, random_state = 42)","metadata":{"execution":{"iopub.status.busy":"2022-02-19T17:00:53.316541Z","iopub.execute_input":"2022-02-19T17:00:53.317294Z","iopub.status.idle":"2022-02-19T17:00:55.599319Z","shell.execute_reply.started":"2022-02-19T17:00:53.317199Z","shell.execute_reply":"2022-02-19T17:00:55.598465Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":"**Sensible Methods**","metadata":{}},{"cell_type":"code","source":"#start with logistic regression\nLog = LogisticRegression(max_iter=10000)\nLog.fit(X_train, Y_train)\nY_pred_log = Log.predict(X_test)\nprint(accuracy_score(Y_test,Y_pred_log))","metadata":{"execution":{"iopub.status.busy":"2022-02-19T17:00:55.600971Z","iopub.execute_input":"2022-02-19T17:00:55.601510Z","iopub.status.idle":"2022-02-19T17:01:15.314558Z","shell.execute_reply.started":"2022-02-19T17:00:55.601463Z","shell.execute_reply":"2022-02-19T17:01:15.311149Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"#use xgb boost for better performance\nxgb = XGBClassifier(eval_metric = 'logloss', use_label_encoder=False)\nxgb.fit(X_train, Y_train)\nY_pred_xgb = xgb.predict(X_test)\nprint(accuracy_score(Y_test,Y_pred_xgb))","metadata":{"execution":{"iopub.status.busy":"2022-02-19T17:01:15.316486Z","iopub.execute_input":"2022-02-19T17:01:15.317124Z","iopub.status.idle":"2022-02-19T17:03:07.022332Z","shell.execute_reply.started":"2022-02-19T17:01:15.317073Z","shell.execute_reply":"2022-02-19T17:03:07.021554Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"#random forest for better performence\nrf = RandomForestClassifier()\nrf.fit(X_train, Y_train)\nY_pred_rf = rf.predict(X_test)\nprint(accuracy_score(Y_test,Y_pred_rf))","metadata":{"execution":{"iopub.status.busy":"2022-02-19T17:03:07.026593Z","iopub.execute_input":"2022-02-19T17:03:07.027527Z","iopub.status.idle":"2022-02-19T17:03:24.671679Z","shell.execute_reply.started":"2022-02-19T17:03:07.027483Z","shell.execute_reply":"2022-02-19T17:03:24.670920Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"markdown","source":"**Error Analysis**","metadata":{}},{"cell_type":"code","source":"#confusion matrix with RF model\n#could work more on false negative\ncm = metrics.confusion_matrix(Y_test, Y_pred_rf)\npd.DataFrame(data = cm, columns = ['Predicted False ', 'Predicted True'],\n            index = ['Actual False', 'Actual True'])","metadata":{"execution":{"iopub.status.busy":"2022-02-19T17:03:24.672909Z","iopub.execute_input":"2022-02-19T17:03:24.673275Z","iopub.status.idle":"2022-02-19T17:03:24.687364Z","shell.execute_reply.started":"2022-02-19T17:03:24.673228Z","shell.execute_reply":"2022-02-19T17:03:24.686666Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"markdown","source":"**What's next?**\n\n1, More feature engineering on the text features, maybe outliers on the numeric features since there is no missing value? With the results of error analysis, we may try to oversampling the positive class? (not very imbalanced but can still try).\n\n2, Consider to fine tune the models and train different features separately using advanced models, usinng ANN to train numeric features, and RNN (LSTM)/MultinomialNB() to train text features, and model ensembling (average the predictions). (This maybe out of the scope of this class)\n\n4, Presentation preparation (polish the notebook with a bit more write up).","metadata":{}}]}